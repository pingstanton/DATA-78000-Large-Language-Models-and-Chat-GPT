{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a51f17aa",
      "metadata": {
        "id": "a51f17aa"
      },
      "source": [
        "## **Ngrams Lab**\n",
        "LLMs and ChatGPT | Fall 2023 | McSweeney | CUNY Graduate Center\n",
        "\n",
        "**Public link to this Google Colab Notebook:\n",
        "https://colab.research.google.com/drive/1HebbqSpe5WXT45j9Oh1y7vOfHk6RO_nw**\n",
        "\n",
        "**Matthew Stanton** | pingstanton@gmail.com | mstanton@gradcenter.cuny.edu | [Lab List on CUNY Academic Commons](https://pingstanton.commons.gc.cuny.edu/2023/09/21/labs-for-data-78000-large-language-models-and-chat-gpt/)\n",
        "\n",
        "**Due:** September 17, 2023; corrections submitted September 21, 2023\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Large Language Models and Chat GPT**\n",
        "*(Mondays 6:30p, Room 5417, CUNY Graduate Center, New York, NY)*\n",
        "\n",
        "Instructor: Michelle McSweeney, [michelleamcsweeney.com](https://michelleamcsweeney.com)\n",
        "\n",
        "Course Site: https://github.com/michellejm/LLMs-fall-23\n",
        "\n",
        "Importing assigned **wordvectors-lab.ipynb** Jupyter workbook from:\n",
        "https://github.com/michellejm/LLMs-fall-23/blob/main/week2-ngrams-tokenizers-wordvectors/ngrams/ngrams-lab.ipynb\n",
        "\n",
        "---\n",
        "\n",
        "This lab is based heavily on the [nltk documentation](https://www.nltk.org/api/nltk.lm.html)\n",
        "\n",
        "Code annotations copied from OpenAI. (2023). ChatGPT (August 3 Version) [Large language model]. https://chat.openai.com"
      ],
      "metadata": {
        "id": "KixOpdvENv1F"
      },
      "id": "KixOpdvENv1F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Background\n",
        "The purpose of this lab is to explore ngram models. Ngram models are a good introduction to language models generally. Language models are probabilistic representations of language. Ngrams have the benefit of being easy to interrogate and relatively easy to understand (as compared to neural networks).\n",
        "\n",
        "In this lab, you will build an ngram model from the corpus of your choosing. The example is with 'The Great Gatsby' from Project Gutenberg, but there's a code block for any text file on your computer  \n",
        "\n",
        "#### Notes\n",
        "This lab is based heavily on the [nltk documentation](https://www.nltk.org/api/nltk.lm.html)"
      ],
      "metadata": {
        "id": "LnYAVoZtNVyd"
      },
      "id": "LnYAVoZtNVyd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Start by loading up the Stanton's usual suspects...\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as scp"
      ],
      "metadata": {
        "id": "czfnxD0sNQ3L"
      },
      "id": "czfnxD0sNQ3L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef35858b",
      "metadata": {
        "id": "ef35858b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006f0ad0-d19c-4aba-bd58-b9ff4b0aae17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "import nltk\n",
        "# if you haven't downloaded punkt before, you only need to run the line below once\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "from nltk.util import bigrams\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7017636",
      "metadata": {
        "id": "e7017636"
      },
      "source": [
        "# Part 1\n",
        "An example of how ngrams are generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8054bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de8054bf",
        "outputId": "7021ff76-3019-491d-82b8-fef72ac6a9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of The Great Gatsby        This ebook is for the use of anyone anywhere\n"
          ]
        }
      ],
      "source": [
        "# you will need to leverage the requests package\n",
        "r = requests.get(r'https://www.gutenberg.org/cache/epub/64317/pg64317.txt')\n",
        "great_gatsby = r.text\n",
        "\n",
        "# first, remove unwanted new line and tab characters from the text\n",
        "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
        "    great_gatsby = great_gatsby.replace(char, \" \")\n",
        "\n",
        "# check\n",
        "print(great_gatsby[:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886ac257",
      "metadata": {
        "id": "886ac257"
      },
      "outputs": [],
      "source": [
        "# remove the metadata at the beginning - this is slightly different for each book\n",
        "great_gatsby = great_gatsby[983:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad39a39e",
      "metadata": {
        "id": "ad39a39e"
      },
      "source": [
        "#### Txt locally\n",
        "If you'd rather use a file on your computer, here's the code -- you just need to save the text file in your local directory, and change the variables throughout.\n",
        "\n",
        "The example is a report from the [Congressional Research Service](https://www.everycrsreport.com/files/2020-11-10_R45178_62d6238caecf6c02ddf495be33b3439f09eed744.pdf) on AI and National Security."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c4c9736",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c4c9736",
        "outputId": "6bbf0e49-b6b2-4307-d481-118328e377eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['in', 'college', 'i', 'was', 'unjustly', 'accused', 'of', 'being', 'a', 'politician', 'because', 'i', 'was', 'privy', 'to', 'the', 'secret', 'griefs', 'of', 'wild', 'unknown', 'men', 'most', 'of', 'the', 'confidences', 'were', 'unsoughtfrequently', 'i', 'have', 'feigned', 'sleep', 'preoccupation', 'or', 'a', 'hostile', 'levity', 'when', 'i', 'realized', 'by', 'some', 'unmistakable', 'sign', 'that', 'an', 'intimate', 'revelation', 'was', 'quivering']\n"
          ]
        }
      ],
      "source": [
        "# this is simplified for demonstration\n",
        "def sample_clean_text(text: str):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove punctuation from text\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "\n",
        "    # tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # return your tokens\n",
        "    return tokens\n",
        "\n",
        "# call the function\n",
        "sample_tokens = sample_clean_text(text = great_gatsby)\n",
        "\n",
        "# check\n",
        "print(sample_tokens[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d828dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94d828dd",
        "outputId": "369022b4-1985-49aa-e844-83b64a7f073c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('in', 'college'),\n",
              " ('college', 'i'),\n",
              " ('i', 'was'),\n",
              " ('was', 'unjustly'),\n",
              " ('unjustly', 'accused'),\n",
              " ('accused', 'of'),\n",
              " ('of', 'being'),\n",
              " ('being', 'a'),\n",
              " ('a', 'politician'),\n",
              " ('politician', 'because')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# create bigrams from the sample tokens\n",
        "my_bigrams = bigrams(sample_tokens)\n",
        "\n",
        "# check\n",
        "list(my_bigrams)[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "239deb54",
      "metadata": {
        "id": "239deb54"
      },
      "source": [
        "# Part 2 - creating an ngram model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From Chat-GPT:**\n",
        "\n",
        "Bigrams are a type of n-gram in natural language processing (NLP) and computational linguistics. N-grams are contiguous sequences of n items (or words) from a given sample of text or speech. In the case of bigrams, n equals 2, so bigrams are sequences of two adjacent words in a text or speech corpus.\n",
        "\n",
        "For example, consider the sentence: \"I love programming in Python.\" In this sentence, the bigrams would be:\n",
        "\n",
        "1. \"I love\"\n",
        "2. \"love programming\"\n",
        "3. \"programming in\"\n",
        "4. \"in Python\"\n",
        "\n",
        "Bigrams are often used in various NLP tasks, including:\n",
        "\n",
        "**Text Analysis:** Bigrams help in understanding the co-occurrence patterns of words. Analyzing bigrams can reveal insights about which words frequently appear together in a given text, which can be useful for tasks like sentiment analysis, text classification, and topic modeling.\n",
        "\n",
        "**Language Modeling:** Bigrams are used in language models to predict the probability of a word based on the previous word. This can be helpful in tasks like speech recognition, machine translation, and text generation.\n",
        "\n",
        "**Information Retrieval:** In information retrieval systems, bigrams are used to index and search for phrases and multi-word expressions.\n",
        "\n",
        "**Text Compression:** Bigrams can be used in text compression algorithms to represent frequently occurring pairs of words more efficiently.\n",
        "\n",
        "In addition to bigrams, there are also **trigrams** (n=3), **4-grams** (n=4), and **n-grams** for larger values of n. The choice of the \"n\" value depends on the specific NLP task and the desired level of granularity in text analysis."
      ],
      "metadata": {
        "id": "408-R9_8PclM"
      },
      "id": "408-R9_8PclM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aba6849",
      "metadata": {
        "id": "9aba6849"
      },
      "outputs": [],
      "source": [
        "# 2 is for bigrams\n",
        "n = 2\n",
        "#specify the text you want to use\n",
        "text = great_gatsby\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be61c83",
      "metadata": {
        "id": "0be61c83"
      },
      "source": [
        "Now we are going to use an NLTK shortcut for preprocessing. This will:\n",
        "* pad all of the sentences with `<s>` and `</s>` to train on sentence boundaries, too.\n",
        "* create both unigrams and bigrams\n",
        "* create a training set and a full vocab to train on\n",
        "\n",
        "We need to give it a pre-tokenized text (we'll use nltk's tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6276da8d",
      "metadata": {
        "id": "6276da8d",
        "outputId": "4617f3ad-1711-4dbb-dc4d-efaf14524943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['then', 'wear', 'the', 'gold', 'hat', ',', 'if', 'that', 'will', 'move', 'her', ';', 'if', 'you', 'can', 'bounce', 'high', ',', 'bounce', 'for', 'her', 'too', ',', 'till', 'she', 'cry', '“', 'lover', ',', 'gold-hatted', ',', 'high-bouncing', 'lover', ',', 'i', 'must', 'have', 'you', '!', '”', 'thomas', 'parke', 'd', '’', 'invilliers', 'i', 'in', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'i', '’', 've', 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since', '.']\n"
          ]
        }
      ],
      "source": [
        "# step 1: tokenize the text into sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "# step 2: tokenize each sentence into words\n",
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# step 3: convert each word to lowercase\n",
        "tokenized_text = [[word.lower() for word in sent] for sent in tokenized_sentences]\n",
        "\n",
        "#notice the sentence breaks and what the first 10 items of the tokenized text\n",
        "print(tokenized_text[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3b7a3de",
      "metadata": {
        "id": "b3b7a3de"
      },
      "source": [
        "Why tokenize sentences and words?\n",
        "We want to be able to retain sentence boundaries to encode that, too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fbc5554",
      "metadata": {
        "id": "9fbc5554",
        "outputId": "1cd108c1-ae4c-4a1c-9131-f216d7028a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Then wear\n"
          ]
        }
      ],
      "source": [
        "# notice what the first 10 items are of the vocabulary\n",
        "print(text[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c306af37",
      "metadata": {
        "id": "c306af37"
      },
      "outputs": [],
      "source": [
        "# we imported this function from nltk\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca61c962",
      "metadata": {
        "id": "ca61c962"
      },
      "outputs": [],
      "source": [
        "from nltk.lm import MLE\n",
        "# we imported this function from nltk linear models (lm)\n",
        "# it is for Maximum Likelihood Estimation\n",
        "\n",
        "# MLE is the model we will use\n",
        "lm = MLE(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ecfc43",
      "metadata": {
        "id": "c3ecfc43",
        "outputId": "81b1b353-2723-4bf8-e4ee-961e966b7446",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# currently the vocab length is 0: it has no prior knowledge\n",
        "len(lm.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141795d8",
      "metadata": {
        "id": "141795d8",
        "outputId": "40c82cea-b30a-4f16-c7db-4238e35fb646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6953"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# fit the model\n",
        "# training data is the bigrams and unigrams\n",
        "# the vocab is all the sentence tokens in the corpus\n",
        "\n",
        "lm.fit(train_data, padded_sents)\n",
        "len(lm.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941458b2",
      "metadata": {
        "id": "941458b2",
        "outputId": "b3a88dbc-4c5b-436b-fd54-4e085ffce4d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('then', 'wear', 'the', 'gold', 'hat', ',', 'if', 'that', 'will', 'move', 'her', ';', 'if', 'you', 'can', 'bounce', 'high', ',', 'bounce', 'for', 'her', 'too', ',', 'till', 'she', 'cry', '“', 'lover', ',', 'gold-hatted', ',', 'high-bouncing', 'lover', ',', 'i', 'must', 'have', 'you', '!', '”', 'thomas', 'parke', 'd', '’', 'invilliers', 'i', 'in', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'i', '’', 've', 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since', '.')\n"
          ]
        }
      ],
      "source": [
        "# inspect the model's vocabulary.\n",
        "# be sure that a sentence you know exists (from tokenized_text) is in the\n",
        "print(lm.vocab.lookup(tokenized_text[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d820f1f0",
      "metadata": {
        "id": "d820f1f0",
        "outputId": "3099819d-753c-4978-e5d8-9fa1ac66308f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('then', 'wear', 'the', 'gold', 'hat', '<UNK>', '.')\n"
          ]
        }
      ],
      "source": [
        "# see what happens when we include a word that is not in the vocab.\n",
        "print(lm.vocab.lookup('then wear the gold hat iphone .'.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0207d680",
      "metadata": {
        "id": "0207d680"
      },
      "source": [
        "What did the model replace 'iphone' with?\n",
        "**UNK**\n",
        "\n",
        "Given that it didn't just return an \"out of vocab\" error, what does that mean about our model?\n",
        "**Our model accounts for unknown words by using the UNK as a placeholder variable.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da9cf2d4",
      "metadata": {
        "id": "da9cf2d4",
        "outputId": "b1a235a3-9c95-4769-c575-5c9a6e71a97f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "183\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0026549057726065954"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# how many times does daisy appear in the model?\n",
        "print(lm.counts['daisy'])\n",
        "\n",
        "# what is the probability of daisy appearing?\n",
        "# this is technically the relative frequency of daisy appearing\n",
        "lm.score('daisy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From Chat-GPT:** In NLTK (Natural Language Toolkit), the **lm.counts()** method is used in the context of language modeling. Specifically, it is used to count the occurrences of n-grams (sequences of n words) within a given text or corpus. This method is part of the NLTK's language modeling module.\n",
        "\n",
        "The **lm.score()** method is used in the context of language modeling to calculate the log-likelihood (log probability) of a given sentence or sequence of words based on a trained language model. This method is used to evaluate how likely a particular sequence of words is according to the language model."
      ],
      "metadata": {
        "id": "2PNKXg8dQZ1P"
      },
      "id": "2PNKXg8dQZ1P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9369bed",
      "metadata": {
        "id": "f9369bed",
        "outputId": "eb6bc594-fee2-4e9f-fa16-8588a9c9efd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07650273224043716"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# how often does (daisy, and) occur and what is the relative frequency?\n",
        "print(lm.counts[['daisy']]['and'])\n",
        "lm.score('and', 'daisy'.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "174f226e",
      "metadata": {
        "id": "174f226e",
        "outputId": "6938b332-4e51-4013-b288-fb3abe37906f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# what is the score of 'UNK'?\n",
        "\n",
        "lm.score(\"<UNK>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d84c0444",
      "metadata": {
        "id": "d84c0444"
      },
      "source": [
        "Does the relative frequency of 'UNK' change your assumption about how the model behaves?\n",
        "**The text and the training data were a close match.**\n",
        "\n",
        "How should we change our model to account for the fact the `<UNK>` words are not accounted for by the model?\n",
        "**Expand or change the training data corpus.**\n",
        "\n",
        "Note: *Programmatically implementing this solution is beyond the scope of this course.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d621b05",
      "metadata": {
        "id": "5d621b05"
      },
      "source": [
        "## Generate text\n",
        "We want to start our sentence with a word, and use that to predict all the words that come after that. We'll specify how long it should be.\n",
        "\n",
        "There is a certain amount of randomness encoded into n-gram models. This prevents a model from becoming entirely deterministic. Maximum Likelihood Estimation without some degree of randomness will only produce the most likely result every time. Setting Random Seed means we will get the same result every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e217db6",
      "metadata": {
        "id": "6e217db6",
        "outputId": "399c6717-4f52-4433-98c5-8b0cfa2af8da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['other', ',', 'but', 'he', 'turned', 'to', 'their', 'cars', 'blocking', 'the', 'dull', 'light', ',', 'and', 'separated', 'only', 'building', 'in', 'the', 'adventitious']\n"
          ]
        }
      ],
      "source": [
        "# generate a 20 word sentence starting with the word, 'daisy'\n",
        "\n",
        "print(lm.generate(20, text_seed= 'daisy', random_seed=42))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e91b7a6",
      "metadata": {
        "id": "8e91b7a6"
      },
      "source": [
        "This next code block is just to clean up the tokenized words and make them easier on human eyes. It is literally a detokenizer, which removes some extraneous text markup and reconciles some words back together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5113674e",
      "metadata": {
        "id": "5113674e"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "def generate_sent(lm, num_words, text_seed, random_seed=42):\n",
        "    \"\"\"\n",
        "    :param model: An ngram language model from `nltk.lm.model`.\n",
        "    :param num_words: Max no. of words to generate.\n",
        "    :param random_seed: Seed value for random.\n",
        "    \"\"\"\n",
        "    content = []\n",
        "    for token in lm.generate(num_words, text_seed=text_seed, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From ChatGPT:** The **TreebankWordDetokenizer** is used for the reverse process of tokenization, which is called detokenization. Detokenization is the process of taking a list of tokens (words or subword units) and reconstructing the original sentence or text from them.\n",
        "\n"
      ],
      "metadata": {
        "id": "yjes14xRRDej"
      },
      "id": "yjes14xRRDej"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2846e34e",
      "metadata": {
        "id": "2846e34e",
        "outputId": "d04bfe28-7c5a-42e0-f7f3-6e0c890d70ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'other, but he turned to their cars blocking the dull light, and separated only building in the adventitious'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# Now generate sentences that look much nicer.\n",
        "generate_sent(lm, 20, text_seed='daisy', random_seed = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "173ad126",
      "metadata": {
        "id": "173ad126"
      },
      "source": [
        "Try a few more sentences, and try out another text. Once you are satisfied with what ngrams can (and cannot) do - post your code to your Github or another site."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(lm, 20, text_seed='Carraways', random_seed = 42)\n",
        "# recognizes Carraways is a name, fails to generate ngrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o_I0r3RzT9rG",
        "outputId": "7b3f5059-c9bd-4a74-a8b4-a7babefea0fb"
      },
      "id": "o_I0r3RzT9rG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'names) 596-1887.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(lm, 20, text_seed='dust', random_seed = 40)\n",
        "# parses \"ll\" - probably from a contraction using \"will\" - as a word?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tKtkX7osUm_w",
        "outputId": "2a4b64cc-202c-48aa-e0a4-ec809a059e78"
      },
      "id": "tKtkX7osUm_w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'know where? ” “ she did. ” “ if all this agreement by that it ’ ll make a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(lm, 20, text_seed='abortive', random_seed = 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HV_aHSfwU7SF",
        "outputId": "ca085d57-e1fa-47ce-9f97-a6a606f37f43"
      },
      "id": "HV_aHSfwU7SF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'in england who gives large uncertain dancing individually or a cricket bat in half a word or not located in'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sent(lm, 20, text_seed='sorrow', random_seed = 50)\n",
        "# I don't understand why it generates the same sentence off a different seed word, although \"abortive sorrow\" are a pair in the source text (end of fourth paragraph in Chapter 1)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lo-Ve1P8VgTC",
        "outputId": "3168ed2e-8c69-4593-eec2-a9de1a7a0d70"
      },
      "id": "Lo-Ve1P8VgTC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'in england who gives large uncertain dancing individually or a cricket bat in half a word or not located in'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}