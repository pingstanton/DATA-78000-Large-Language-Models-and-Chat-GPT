{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pingstanton/DATA-78000-Large-Language-Models-and-Chat-GPT/blob/main/Word_Vectors_Lab_Stanton_for_78000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Vectors Lab**\n",
        "LLMs and ChatGPT | Fall 2023 | McSweeney | CUNY Graduate Center\n",
        "\n",
        "**Public link to this Google Colab Notebook:\n",
        "https://colab.research.google.com/drive/1B2Qy5AzfZEp_wF34yW4Z8S82lF6LtFbT**\n",
        "\n",
        "**Matthew Stanton** | pingstanton@gmail.com | mstanton@gradcenter.cuny.edu | [Lab List on CUNY Academic Commons](https://pingstanton.commons.gc.cuny.edu/2023/09/21/labs-for-data-78000-large-language-models-and-chat-gpt/)\n",
        "\n",
        "**Due:** September 25? Or next class after Yom?"
      ],
      "metadata": {
        "id": "GZ1jaPIO1Ji7"
      },
      "id": "GZ1jaPIO1Ji7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Large Language Models and Chat GPT**\n",
        "*(Mondays 6:30p, Room 5417, CUNY Graduate Center, New York, NY)*\n",
        "\n",
        "Instructor: Michelle McSweeney, [michelleamcsweeney.com](https://michelleamcsweeney.com)\n",
        "\n",
        "Course Site: https://github.com/michellejm/LLMs-fall-23\n",
        "\n",
        "Importing assigned **wordvectors-lab.ipynb** Jupyter workbook from:\n",
        "https://github.com/michellejm/LLMs-fall-23/blob/main/week4-tokenization-word%20vectors/word2vec/wordvectors-lab.ipynb\n",
        "\n",
        "---\n",
        "\n",
        "This lab is based heavily on the [nltk documentation](https://www.nltk.org/api/nltk.lm.html)\n",
        "\n",
        "Code annotations copied from OpenAI. (2023). ChatGPT (August 3 Version) [Large language model]. https://chat.openai.com"
      ],
      "metadata": {
        "id": "rGdM8cahH5Rh"
      },
      "id": "rGdM8cahH5Rh"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9a003aa2",
      "metadata": {
        "id": "9a003aa2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Since you are using Google Colab, don't forget to add nltk.download('punkt')**\n",
        "\n",
        "The line `nltk.download('punkt')` is a Python command that downloads the NLTK (Natural Language Toolkit) data package named \"**punkt**.\" \"Punkt\" is pronounced as \"poongkt.\" It rhymes with \"spooked\" but with a \"p\" sound at the beginning. It is a German word that means \"point\" or \"dot\" and is used in various contexts, including in the name of the NLTK package \"punkt,\" which is used for natural language processing tasks like tokenization.\n",
        "\n",
        "In NLTK, \"punkt\" refers to a **pre-trained tokenization model** (pretrained + UNK + token). Tokenization is the process of splitting a text into individual words or tokens.\n",
        "\n",
        "The '**punkt**' package includes data files and models that NLTK uses for tokenization tasks, such as breaking down a paragraph of text into sentences or sentences into words.\n",
        "\n",
        "When you execute `nltk.download('punkt')`, it retrieves and installs this package from the NLTK data repository if you haven't already downloaded it. Downloading it is necessary before using NLTK's tokenization functions, as they rely on the data provided by this package.\n",
        "\n",
        "Since you're using Google Colab instead of a machine-based instance of Jupyter Notebook, you might need to run this command when working with NLTK for the first time in a session to ensure you have the required data downloaded. Once you've downloaded it, you don't need to do it again in the same session."
      ],
      "metadata": {
        "id": "tOvOVcbEA77h"
      },
      "id": "tOvOVcbEA77h"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXc-HcQn4HpG",
        "outputId": "67d57155-f1c0-42cf-ed51-3862897a9cfa"
      },
      "id": "gXc-HcQn4HpG",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Python, **import requests** is used to import the requests library, which is a popular third-party library for making HTTP requests. The requests library simplifies the process of sending HTTP requests to web services, APIs, or websites and handling the responses.\n",
        "\n",
        "In this case, since you like to call your data files from the chimaboo.com server, you need to import and assign the file to the variable Prof. McSweeney is using for the remainder of the tolken lab script..."
      ],
      "metadata": {
        "id": "F3Z4oUexE5Tw"
      },
      "id": "F3Z4oUexE5Tw"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "result = requests.get('https://chimaboo.com/coursework/DATA78000/hunger_games.txt')\n",
        "file = result.text"
      ],
      "metadata": {
        "id": "wdEtSptcDWeI"
      },
      "id": "wdEtSptcDWeI",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "print(file[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtjtVnOODbX-",
        "outputId": "de1a4216-5ada-4131-8ca1-be52115f22e0"
      },
      "id": "mtjtVnOODbX-",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Second Book of THE HUNGER GAMES \n",
            "\n",
            "\n",
            "\n",
            "New York Times Bestsel ling Author \n",
            "\n",
            "SUZHNNE \n",
            "COLLINS \n",
            "\n",
            "\n",
            "\n",
            "PA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and now back to Prof. McSweeney's script as posted on GitHub:"
      ],
      "metadata": {
        "id": "NBPuHbU7Fb26"
      },
      "id": "NBPuHbU7Fb26"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "df51b1f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df51b1f2",
        "outputId": "87385c1f-3f66-451e-bb81-fd4ddb96012c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Second Book of THE HUNGER GAMES     New York Times Bestsel ling Author   SUZHNNE  COLLINS     PA\n"
          ]
        }
      ],
      "source": [
        "# first, remove unwanted new line and tab characters from the text\n",
        "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
        "    file = file.replace(char, \" \")\n",
        "\n",
        "# check again\n",
        "print(file[:100])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script above is intended to clean a text stored in the file variable by replacing certain characters with spaces.\n",
        "\n",
        "```\n",
        "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
        "```\n",
        "\n",
        "This line starts a loop that iterates over a list of characters: \"\\n\" (newline), \"\\r\" (carriage return), \"\\d\" (the character 'd'), and \"\\t\" (tab).\n",
        "\n",
        "```\n",
        "file = file.replace(char, \" \"):\n",
        "```\n",
        "\n",
        "For each character in the list, this line replaces all occurrences of that character in the file string with a space character \" \". It assigns the modified string back to the file variable."
      ],
      "metadata": {
        "id": "J-PA47iAJmRE"
      },
      "id": "J-PA47iAJmRE"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "089918f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "089918f1",
        "outputId": "2cb4fb26-32d5-4c87-cdda-67b403c3f00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['the', 'second', 'book', 'of', 'the', 'hunger', 'games', 'new', 'york', 'times', 'bestsel', 'ling', 'author', 'suzhnne', 'collins', 'parti', '``', 'the', 'spark', \"''\", '2', '|', 'p', 'a', 'g', 'e', 'catching', 'fire', '-', 'suzanne', 'collins', 'i', 'clasp', 'the', 'flask', 'between', 'my', 'hands', 'even', 'though', 'the', 'warmth', 'from', 'the', 'tea', 'has', 'long', 'since', 'leached', 'into', 'the', 'frozen', 'air', '.'], ['my', 'muscles', 'are', 'clenched', 'tight', 'against', 'the', 'cold', '.'], ['if', 'a', 'pack', 'of', 'wild', 'dogs', 'were', 'to', 'appear', 'at', 'this', 'moment', ',', 'the', 'odds', 'of', 'scaling', 'a', 'tree', 'before', 'they', 'attacked', 'are', 'not', 'in', 'my', 'favor', '.'], ['i', 'should', 'get', 'up', ',', 'move', 'around', ',', 'and', 'work', 'the', 'stiffness', 'from', 'my', 'limbs', '.'], ['but', 'instead', 'i', 'sit', ',', 'as', 'motionless', 'as', 'the', 'rock', 'beneath', 'me', ',', 'while', 'the', 'dawn', 'begins', 'to', 'lighten', 'the', 'woods', '.'], ['i', 'ca', \"n't\", 'fight', 'the', 'sun', '.'], ['i', 'can', 'only', 'watch', 'helplessly', 'as', 'it', 'drags', 'me', 'into', 'a', 'day', 'that', 'i', \"'ve\", 'been', 'dreading', 'for', 'months', '.'], ['by', 'noon', 'they', 'will', 'all', 'be', 'at', 'my', 'new', 'house', 'in', 'the', 'victor', \"'s\", 'village', '.'], ['the', 'reporters', ',', 'the', 'camera', 'crews', ',', 'even', 'effie', 'trinket', ',', 'my', 'old', 'escort', ',', 'will', 'have', 'made', 'their', 'way', 'to', 'district', '12', 'from', 'the', 'capitol', '.'], ['i', 'wonder', 'if', 'effie', 'will', 'still', 'be', 'wearing', 'that', 'silly', 'pink', 'wig', ',', 'or', 'if', 'she', \"'ll\", 'be', 'sporting', 'some', 'other', 'unnatural', 'color', 'especially', 'for', 'the', 'victory', 'tour', '.'], ['there', 'will', 'be', 'others', 'waiting', ',', 'too', '.'], ['a', 'staff', 'to', 'cater', 'to', 'my', 'every', 'need', 'on', 'the', 'long', 'train', 'trip', '.'], ['a', 'prep', 'team', 'to', 'beautify', 'me', 'for', 'public', 'appearances', '.'], ['my', 'stylist', 'and', 'friend', ',', 'cinna', ',', 'who', 'designed', 'the', 'gorgeous', 'outfits', 'that', 'first', 'made', 'the', 'audience', 'take', 'notice', 'of', 'me', 'in', 'the', 'hunger', 'games', '.'], ['if', 'it', 'were', 'up', 'to', 'me', ',', 'i', 'would', 'try', 'to', 'forget', 'the', 'hunger', 'games', 'entirely', '.'], ['never', 'speak', 'of', 'them', '.'], ['pretend', 'they', 'were', 'nothing', 'but', 'a', 'bad', 'dream', '.'], ['but', 'the', 'victory', 'tour', 'makes', 'that', 'impossible', '.'], ['strategically', 'placed', 'almost', 'midway', 'between', 'the', 'annual', 'games', ',', 'it', 'is', 'the', 'capitol', \"'s\", 'way', 'of', 'keeping', 'the', 'horror', 'fresh', 'and', 'immediate', '.'], ['not', 'only', 'are', 'we', 'in', 'the', 'districts', 'forced', 'to', 'remember', 'the', 'iron', 'grip', 'of', 'the', 'capitol', \"'s\", 'power', 'each', 'year', ',', 'we', 'are', 'forced', 'to', 'celebrate', 'it', '.'], ['and', 'this', 'year', ',', 'i', 'am', 'one', 'of', 'the', '3', '|', 'p', 'a', 'g', 'e', 'catching', 'fire', '-', 'suzanne', 'collins', 'stars', 'of', 'the', 'show', '.'], ['i', 'will', 'have', 'to', 'travel', 'from', 'district', 'to', 'district', ',', 'to', 'stand', 'before', 'the', 'cheering', 'crowds', 'who', 'secretly', 'loathe', 'me', ',', 'to', 'look', 'down', 'into', 'the', 'faces', 'of', 'the', 'families', 'whose', 'children', 'i', 'have', 'killed', '...'], ['the', 'sun', 'persists', 'in', 'rising', ',', 'so', 'i', 'make', 'myself', 'stand', '.'], ['all', 'my', 'joints', 'complain', 'and', 'my', 'left', 'leg', 'has', 'been', 'asleep', 'for', 'so', 'long', 'that', 'it', 'takes', 'several', 'minutes', 'of', 'pacing', 'to', 'bring', 'the', 'feeling', 'back', 'into', 'it', '.'], ['i', \"'ve\", 'been', 'in', 'the', 'woods', 'three', 'hours', ',', 'but', 'as', 'i', \"'ve\", 'made', 'no', 'real', 'attempt', 'at', 'hunting', ',', 'i', 'have', 'nothing', 'to', 'show', 'for', 'it', '.'], ['it', 'does', \"n't\", 'matter', 'for', 'my', 'mother', 'and', 'little', 'sister', ',', 'prim', ',', 'anymore', '.'], ['they', 'can', 'afford', 'to', 'buy', 'butcher', 'meat', 'in', 'town', ',', 'although', 'none', 'of', 'us', 'likes', 'it', 'any', 'better', 'than', 'fresh', 'game', '.'], ['but', 'my', 'best', 'friend', ',', 'gale', 'hawthorne', ',', 'and', 'his', 'family', 'will', 'be', 'depending', 'on', 'today', \"'s\", 'haul', 'and', 'i', 'ca', \"n't\", 'let', 'them', 'down', '.'], ['i', 'start', 'the', 'hour-and-a-half', 'trek', 'it', 'will', 'take', 'to', 'cover', 'our', 'snare', 'line', '.'], ['back', 'when', 'we', 'were', 'in', 'school', ',', 'we', 'had', 'time', 'in', 'the', 'afternoons', 'to', 'check', 'the', 'line', 'and', 'hunt', 'and', 'gather', 'and', 'still', 'get', 'back', 'to', 'trade', 'in', 'town', '.'], ['but', 'now', 'that', 'gale', 'has', 'gone', 'to', 'work', 'in', 'the', 'coal', 'mines', 'â\\x80\\x94', 'and', 'i', 'have', 'nothing', 'to', 'do', 'all', 'day', 'â\\x80\\x94', 'i', \"'ve\", 'taken', 'over', 'the', 'job', '.'], ['by', 'this', 'time', 'gale', 'will', 'have', 'clocked', 'in', 'at', 'the', 'mines', ',', 'taken', 'the', 'stomach-churning', 'elevator', 'ride', 'into', 'the', 'depths', 'of', 'the', 'earth', ',', 'and', 'be', 'pounding', 'away', 'at', 'a', 'coal', 'seam', '.'], ['i', 'know', 'what', 'it', \"'s\", 'like', 'down', 'there', '.'], ['every', 'year', 'in', 'school', ',', 'as', 'part', 'of', 'our', 'training', ',', 'my', 'class', 'had', 'to', 'tour', 'the', 'mines', '.'], ['when', 'i', 'was', 'little', ',', 'it', 'was', 'just', 'unpleasant', '.'], ['the', 'claustrophobic', 'tunnels', ',', 'foul', 'air', ',', 'suffocating', 'darkness', 'on', 'all', 'sides', '.'], ['but', 'after', 'my', 'father', 'and', 'several', 'other', 'miners', 'were', 'killed', 'in', 'an', 'explosion', ',', 'i', 'could', 'barely', 'force', 'myself', 'onto', 'the', 'elevator', '.'], ['the', 'annual', 'trip', 'became', 'an', 'enormous', 'source', 'of', 'anxiety', '.'], ['twice', 'i', 'made', 'myself', 'so', 'sick', 'in', 'anticipation', 'of', 'it', 'that', 'my', 'mother', 'kept', 'me', 'home', 'because', 'she', 'thought', 'i', 'had', 'contracted', 'the', 'flu', '.'], ['4', '|', 'p', 'a', 'g', 'e', 'catching', 'fire', '-', 'suzanne', 'collins', 'i', 'think', 'of', 'gale', ',', 'who', 'is', 'only', 'really', 'alive', 'in', 'the', 'woods', ',', 'with', 'its', 'fresh', 'air', 'and', 'sunlight', 'and', 'clean', ',', 'flowing', 'water', '.'], ['i', 'do', \"n't\", 'know', 'how', 'he', 'stands', 'it', '.'], ['well', '...', 'yes', ',', 'i', 'do', '.'], ['he', 'stands', 'it', 'because', 'it', \"'s\", 'the', 'way', 'to', 'feed', 'his', 'mother', 'and', 'two', 'younger', 'brothers', 'and', 'sister', '.'], ['and', 'here', 'i', 'am', 'with', 'buckets', 'of', 'money', ',', 'far', 'more', 'than', 'enough', 'to', 'feed', 'both', 'our', 'families', 'now', ',', 'and', 'he', 'wo', \"n't\", 'take', 'a', 'single', 'coin', '.'], ['it', \"'s\", 'even', 'hard', 'for', 'him', 'to', 'let', 'me', 'bring', 'in', 'meat', ',', 'although', 'he', \"'d\", 'surely', 'have', 'kept', 'my', 'mother', 'and', 'prim', 'supplied', 'if', 'i', \"'d\", 'been', 'killed', 'in', 'the', 'games', '.'], ['i', 'tell', 'him', 'he', \"'s\", 'doing', 'me', 'a', 'favor', ',', 'that', 'it', 'drives', 'me', 'nuts', 'to', 'sit', 'around', 'all', 'day', '.'], ['even', 'so', ',', 'i', 'never', 'drop', 'off', 'the', 'game', 'while', 'he', \"'s\", 'at', 'home', '.'], ['which', 'is', 'easy', 'since', 'he', 'works', 'twelve', 'hours', 'a', 'day', '.'], ['the', 'only', 'time', 'i', 'really', 'get', 'to', 'see', 'gale', 'now', 'is', 'on', 'sundays', ',', 'when', 'we', 'meet', 'up', 'in', 'the', 'woods', 'to', 'hunt', 'together', '.'], ['it', \"'s\", 'still', 'the', 'best', 'day', 'of', 'the', 'week', ',', 'but', 'it', \"'s\", 'not', 'like', 'it', 'used', 'to', 'be', 'before', ',', 'when', 'we', 'could', 'tell', 'each', 'other', 'anything', '.']]\n"
          ]
        }
      ],
      "source": [
        "# this is simplified for demonstration\n",
        "def sample_clean_text(text: str):\n",
        "    # step 1: tokenize the text into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # step 2: tokenize each sentence into words\n",
        "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # step 3: convert each word to lowercase\n",
        "    tokenized_text = [[word.lower() for word in sent] for sent in tokenized_sentences]\n",
        "\n",
        "    # return your tokens\n",
        "    return tokenized_text\n",
        "\n",
        "# call the function\n",
        "tokens = sample_clean_text(text = file)\n",
        "\n",
        "# check\n",
        "print(tokens[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The script above is intended to clean a text stored in the file variable by replacing certain characters with spaces. Here's what each part of the script does:\n",
        "\n",
        "\n",
        "```\n",
        "for char in [\"\\n\", \"\\r\", \"\\d\", \"\\t\"]:\n",
        "    file = file.replace(char, \" \")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pBt0mNCqFyYj"
      },
      "id": "pBt0mNCqFyYj"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a0d76ab9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0d76ab9",
        "outputId": "161b5640-5cda-4ea8-844c-8f7d765e1cb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.19487983,  0.2700533 , -1.5583338 ,  0.36597925, -0.3371598 ,\n",
              "       -0.9551392 ,  0.9198467 ,  0.5818802 , -0.71432555, -0.13867426,\n",
              "        0.79693824, -1.4810411 ,  0.15073562,  0.8626713 , -0.14752384,\n",
              "        0.70137936,  1.3576615 , -0.29934433, -0.80562323, -0.74933475,\n",
              "        0.5721515 , -0.12363323,  0.04184139,  0.15163137,  0.7772465 ,\n",
              "        0.18112125,  0.00838861,  0.6172922 , -0.6277335 ,  0.7451277 ,\n",
              "        0.7468352 ,  0.29427582, -0.4461629 ,  0.05755919, -0.89721066,\n",
              "       -0.00350799,  1.0337962 ,  0.5536823 , -0.08614762, -0.10726095,\n",
              "       -0.18539508,  0.3544069 ,  0.5748329 , -0.0603789 , -0.14828049,\n",
              "        0.4158328 ,  0.00622626, -0.50676584, -0.72197056,  0.65061384],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model = Word2Vec(tokens,vector_size=50)\n",
        "model.wv[\"the\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line of code above is creating a **Word2Vec model** using the Gensim library for natural language processing. Specifically, it is creating a Word2Vec model with the following characteristics:\n",
        "\n",
        "**tokens:** This is expected to be a list of sentences or documents that you want to use for training the Word2Vec model. Each sentence or document should be represented as a list of words or tokens. The Word2Vec model learns word embeddings (vector representations) based on the input tokens.\n",
        "\n",
        "**vector_size:** This parameter specifies the dimensionality of the word vectors that the Word2Vec model will generate. In this case, the vectors will have 50 dimensions. You can adjust this number based on your specific requirements, but common choices include 100, 200, or 300 dimensions for more complex models.\n",
        "\n",
        "Here's a breakdown of what this code does:\n",
        "\n",
        "```\n",
        "model = Word2Vec(tokens, vector_size=50)\n",
        "```\n",
        "\n",
        "**tokens** should be a list of sentences or documents, where each sentence/document is represented as a list of words or tokens.\n",
        "\n",
        "**vector_size** specifies the dimensionality of the word vectors.\n",
        "\n",
        "After creating the Word2Vec model, you can use it to perform various natural language processing tasks, such as finding similar words, word analogies, or generating word embeddings for downstream machine learning tasks like text classification or sentiment analysis. The model will have learned vector representations for the words in the input data that capture semantic relationships between words based on their co-occurrence patterns in the training data.\n",
        "\n",
        "Now, taking both lines together...\n",
        "\n",
        "```\n",
        "model = Word2Vec(tokens,vector_size=50)\n",
        "model.wv[\"the\"]\n",
        "```\n",
        "\n",
        "**model = Word2Vec(tokens, vector_size=50):** This line creates a Word2Vec model using the Gensim library. It takes a list of sentences or documents represented as lists of words or tokens (tokens) and trains a Word2Vec model with word vectors of 50 dimensions (vector_size=50).\n",
        "\n",
        "**model.wv[\"the\"]:** After creating the Word2Vec model, this line accesses the word vector for the word \"the.\" Specifically, it uses the wv attribute of the Word2Vec model to access the word vectors, and [\"the\"] is used to specify the word for which you want to retrieve the vector.\n",
        "\n",
        "So, model.wv[\"the\"] retrieves the word vector for the word \"the\" as learned by the Word2Vec model. This vector is a numerical representation of the word's meaning in the context of the training data. The resulting vector is a 1D NumPy array with 50 elements since we specified a vector size of 50 when creating the model.\n",
        "\n",
        "You can use these word vectors for various natural language processing tasks, such as finding similar words, word analogies, or as input features for machine learning models.\n"
      ],
      "metadata": {
        "id": "c6iR_0uOKpfJ"
      },
      "id": "c6iR_0uOKpfJ"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "22431ef8",
      "metadata": {
        "id": "22431ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9f9dda-f7d9-419f-ab4e-0a2fb5bc9add"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00828638,  0.12144296, -0.16573092,  0.10531649,  0.0018127 ,\n",
              "       -0.30760288,  0.5414007 ,  0.48497337, -0.34711483, -0.14518443,\n",
              "        0.11741909, -0.67270654,  0.18725881,  0.26036954, -0.08415695,\n",
              "        0.30421323,  0.1973696 ,  0.10552846, -0.526346  , -0.26913697,\n",
              "        0.02794474,  0.26113355,  0.4140011 , -0.02001527,  0.23870245,\n",
              "        0.05185207, -0.14531691,  0.09307087, -0.37051168,  0.1734845 ,\n",
              "        0.10637498, -0.03884476, -0.25154987, -0.18385673, -0.39669213,\n",
              "        0.21242139,  0.35123512,  0.07836144,  0.02506723, -0.03897581,\n",
              "        0.27363548, -0.02549281, -0.11196419, -0.0692548 ,  0.43600735,\n",
              "        0.23855945, -0.15036625, -0.2789507 ,  0.13013215,  0.19248243],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = Word2Vec(tokens,vector_size=50)\n",
        "model.wv[\"run\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9df90eb8",
      "metadata": {
        "id": "9df90eb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168127ca-d067-4adf-94a5-ceac1c86f8b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03973578,  0.08627009,  0.27648053,  0.10672029, -0.03928645,\n",
              "       -0.26195595,  0.67079884,  0.7193839 , -0.34723112, -0.3836738 ,\n",
              "       -0.00602278, -0.6395669 ,  0.22696626,  0.2341876 , -0.20746003,\n",
              "        0.5408073 , -0.00947533,  0.24765918, -0.6678093 , -0.324585  ,\n",
              "       -0.01951198,  0.52252084,  0.7232437 , -0.22652604,  0.14861163,\n",
              "        0.06890932, -0.34551346, -0.10802484, -0.48439983,  0.05078671,\n",
              "        0.04133791, -0.22455572, -0.25076136, -0.1977682 , -0.3584602 ,\n",
              "        0.24895051,  0.3140884 ,  0.00397272,  0.0087159 , -0.05739469,\n",
              "        0.6595537 , -0.26881585, -0.3792112 , -0.06300899,  0.74578094,\n",
              "        0.19023395, -0.05808637, -0.50393695,  0.41929024,  0.16718782],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model = Word2Vec(tokens,vector_size=50)\n",
        "model.wv[\"katniss\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(tokens,vector_size=50)\n",
        "model.wv[\"snow\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83oVWVA78_kb",
        "outputId": "aca7e3ca-4db1-415e-d038-ef6835804b44"
      },
      "id": "83oVWVA78_kb",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.2528978e-02,  1.8991829e-01, -4.2667651e-01,  2.8951898e-01,\n",
              "       -7.2996458e-04, -5.8234596e-01,  8.8482010e-01,  7.8967333e-01,\n",
              "       -5.9726334e-01, -9.6648030e-02,  2.8540772e-01, -1.2258208e+00,\n",
              "        4.0395173e-01,  4.5815641e-01, -1.7876726e-01,  4.9953991e-01,\n",
              "        5.4074347e-01,  1.2455404e-01, -8.2913208e-01, -4.9192691e-01,\n",
              "        7.7119984e-02,  4.0942207e-01,  5.3532690e-01, -4.8470806e-02,\n",
              "        3.6675662e-01,  1.1739686e-01, -2.8019133e-01,  1.8026008e-01,\n",
              "       -5.5397397e-01,  2.2549400e-01,  1.2846416e-01,  7.3842322e-03,\n",
              "       -4.8193315e-01, -1.5609476e-01, -6.4691317e-01,  2.1228524e-01,\n",
              "        7.4888295e-01,  3.1401318e-01, -4.9478617e-02,  5.8001623e-02,\n",
              "        2.9869145e-01, -1.0381807e-01,  2.1671252e-02, -7.7230014e-02,\n",
              "        6.9380474e-01,  3.6063766e-01, -2.2480157e-01, -4.5635536e-01,\n",
              "        1.9147922e-01,  4.3084514e-01], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}